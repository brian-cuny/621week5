---
output:
  pdf_document:
    df_print: kable
header-includes:
  \usepackage{fancyhdr}
  \pagestyle{fancy}
  \fancyhead[L]{\textbf{Wine Purchase Regression}}
  \fancyhead[R]{\thepage}
  \fancyfoot[C]{}
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(magrittr)
library(knitr)
library(caret)
library(car)
library(mice)
library(lars)
library(pscl)
library(vcd)
library(MASS)
library(AICcmodavg)
library(corrplot)
wine <- read_csv('C:\\Users\\Brian\\Desktop\\GradClasses\\Summer18\\621\\621week5\\wine-training-data.csv') %>%
  dplyr::select(-INDEX) %>%
  mutate(LabelAppeal = factor(LabelAppeal),
         STARS = factor(STARS))
```

#Abstract

I have been tasked with creating 6 regressions in an effort to predict the number of cases of wine purchased based on a number of predictors about the wine. The response variable is a count of the number of cases of wine purchased. There are 12795 observations, 14 predictors and 1 response variable. 

I will begin by analysizing the initial raw data and then perform 2 multiple linear regressions, 2 poisson regression and 2 negative binomial regressions. From there I will select the best model to deploy.

#Data Preparation

The raw data was read in and after an initial inspection only a few small modification needed to be made. I removed the INDEX column as it was unnecessary and modified LabelAppeal and STARS to be factors as indicated by their descriptions. The rest of the predictors are continuous or discrete.

From there I explored the number of missing values in the data set.

```{r, cache=TRUE}
wine %>%
  map_dbl(~sum(is.na(.))/nrow(wine)) %>%
  kable()
```

There are a number of missing values in the data set but most of them comprise a relatively small proportion of the total observations. This should not be problematic to deal with. However, I am missing more than a quarter of all the STARS rankings. This is a sizeable amount of data. My first intuition is to remove this predictor but there is reason to believe that it will be highly significant as a predictor. There is strong evidence to suggest that people are not all that knowlege about wine purchases and will largely base their decisions on arguments from authority. Therefore, an external ranking system seems important. There is also another problem exhibited by the `aggr` plot

```{r cache=TRUE, warning=FALSE}
VIM::aggr(wine[, -1], col=c('navyblue', 'yellow'),
          numbers=TRUE, sortVars=TRUE, 
          labels=names(wine[, -1]), cex.axis=.7,
          gap=3, ylab=c('Missing Data', 'Pattern'), combined=TRUE)
```

The above plot highlights all the missing values. As expected the two largest groups include the data with nothing missing followed by data only missing STARS. The problem, however, are most of the other observations. Although any given predictor is missing fewer than 10% of it's data, those missing observations are clumped together. That is, there are many observations that are missing multiple pieces of data. This is problematic as I do not feel comfortable imputed nearly half of the predictors for a wine. Imputation is powerful, but it is not magical.

As a result I decide to remove observations missing 2 or more predcitors and impute the rest of the data. The choice for cutoff is somewhat arbitrary but I feel as if it gives a good balance of keeping as many observations as possible without imputing too much missing data. The proportion of observations missing 2 or more predictors is also a small proportion of the total number of observations so I will not be losing too much valuable data.

```{r cache=TRUE}
temp.wine <- wine %>%
  filter(rowSums(is.na(wine)) <= 1) 

VIM::aggr(temp.wine[, -1], col=c('navyblue', 'yellow'),
          numbers=TRUE, sortVars=TRUE, 
          labels=names(temp.wine[, -1]), cex.axis=.7,
          gap=3, ylab=c('Missing Data', 'Pattern'), combined=TRUE)
```

With the data missing multiple predictors removed, I am ready to impute the data. In total, only about 10% of the observations were removed. 

```{r cache=TRUE, warning=FALSE}
set.seed(123)
imputed.data <- mice::mice(temp.wine[, -1], m=5, maxit=50, method='pmm', seed=500, printFlag=FALSE)
wine.complete <- cbind(temp.wine[, 1], complete(imputed.data, 1))
```

From there I create a training and testing partition to test my various models. 

```{r cache=TRUE}
set.seed(1)
part <- caret::createDataPartition(wine.complete$TARGET, p=0.8, list=FALSE)
wine.training <- wine.complete %>%
  filter(row_number() %in% part)
wine.testing <- wine.complete %>%
  filter(!row_number() %in% part)
```

#Data Exploration

With the data imputed and seperated, I am ready to begin my data exploration. The first plot demonstrates the odd distribution of the response variable. A poisson distribution rquires an equal mean and variance and this data does not appear to have that. In fact, the disproportionately high number of 0s indicate that most wines are not purchased, however, if the wine is purchased it is for an average of 4 cases. That is, there is a binomial question of 0 cases vs. 1 or more cases and a poission question of how many cases, given that at least 1 case is purchased. 

We will keep this in mind during the model analysis.

```{r cache=TRUE, fig.width=4, fig.height=4, fig.align='center'}
ggplot(wine.training, aes(TARGET)) +
  geom_bar() +
  labs(x='Cases Purchased',
       y='Frequency',
       title='Distribution of Cases Purchased')
```

Correlation between the predictors does not appear to be an issue so it is unlikely that will need to addressed in our models.

```{r cache=TRUE, fig.width=4, fig.height=4, fig.align='center'}
corrplot(cor(wine.training[, c(-1, -13, -15)]), method='circle')
```

#Multiple Linear Regression

In general, when working with count data a poisson regression is prefered. However, if the count is large enough a multiple linear regression is a valid substitution. Given that the TARGET count in this case only ranges from 0 to 8 it seems unlikely that a multiple linear regression will be sufficient. It may be useful though to compare these models to the other regressions.

##Model 1

The first model is be created by initially supplying all the predictors and then using `stepAIC` to select the needed predictors

```{r}
lm.1 <- lm(TARGET ~ ., data=wine.training)
MASS::stepAIC(lm.1, trace=0)
lm.1 <- update(lm.1, . ~ . -FixedAcidity -CitricAcid -ResidualSugar)
summary(lm.1)
```

The diagnostics [SEE APPENDIX] have many causes for concern. The most immediately obvious issue is the heteroscedasticity seen in the variance and the lack of a normal fit on the qqplot. Transformational tools are problematic as well due to the fact that the response variable has 0s.

##Model 2

For the 2nd model I will use BIC as a predictor selection method instead of AIC. BIC has a stronger penalty for predictors. This may help create a more simple and interpretable model. [SEE APPENDIX]

```{r cache=TRUE}
lm.2 <- lm(TARGET ~ ., wine.training)
stepAIC(lm.2, k=log(nrow(wine.training)), trace=0)
lm.2 <- update(lm.2, . ~ . -FixedAcidity -ResidualSugar -Density -pH -Sulphates, wine.training)
summary(lm.2)
```

```{r cache=TRUE}
anova(lm.1, lm.2, test='Chisq')
```

There appears to be a statistically significant difference between the two models. The BIC selection method may have left the model is nearly all significant predictors, but it apparently performs statistically worse than the first model. This may indicate overfitting on the first model or that the removed predictors had some predictive value when taken together.

#Poisson Regression

##Model 3

Poisson regressions are best for count data like the TARGET response variable in this data set. However, there is a limitation to the poisson regression in that the mean and variance are equal. There is the possibility for overdispersion in real world data. I will need to examine that in the models.

For model 3 I used a poisson regression and selected the predictors using lasso. [SEE APPENDIX].

```{r cache=TRUE}
lm.3 <- glm(TARGET ~ . -FixedAcidity, wine.training, family=poisson)
summary(lm.3)
```

Examining the model indicates a very poor fit. Calculating the dispersion parameter indicates that dispersion does not appear to be a problem however. To be sure, I will fit a quasipoisson regression.

```{r cache=TRUE}
pchisq(lm.3$deviance, df=lm.3$df.residual, lower.tail=FALSE)
```

##Model 4

For the second poisson regression I will attempt to remove several of the non-statistically significant predictors. The lasso method for the previous model left numerous predictors that the other models found to not be significant and that exploratory analysis indicated did not have strong predictive power. I will attempt to simplify the model without losing predictive ability. [SEE APPENDIX]

```{r cache=TRUE}
lm.4 <- glm(TARGET ~ . -FixedAcidity -CitricAcid -ResidualSugar -Density -Sulphates, 
            wine.training, family=poisson)
summary(lm.4)
```

```{r cache=TRUE}
anova(lm.3, lm.4, test='Chisq')
```

The anova test indicates that the more simple model is a valid substitution for the previous model without causing any lost of predictive ability.

```{r cache=TRUE}
pchisq(lm.4$deviance, df=lm.4$df.residual, lower.tail=FALSE)
```

However, like the previous model this model still appears to be a poor fit. 

#Negative Binomial

##Model 5

When a model appears as if it should follow a poisson distribution but the poisson model does not appear to be an appropriate fit, a negative biniomial model may address the underlying issues. I will fit a negative binomial model and select the predictors using `stepAIC`.

```{r cache=TRUE, warning=FALSE}
lm.5 <- glm.nb(TARGET ~ ., data=wine.training)
stepAIC(lm.5, trace=0)
lm.5 <- update(lm.5, . ~ . -FixedAcidity -ResidualSugar)
summary(lm.5)
```

The model appears to suffer from a similar slate of issues in the diagnostics are in the previous models. [SEE APPENDIX]

##Model 6

Exploring the TARGET in the training data reveals a disproportionately large amount of 0 values. 0s make up more than 20% of the total data, nearly double the proportional amount. This indicates that there is a hurdle in the number of cases purchased. That is, most wines have 0 purchases but amongst wines that have been purchased there is a fairly normal distribution.

```{r cache=TRUE}
wine.training %>% 
  group_by(TARGET) %>%
  count()
```

For the last model, I will fit a hurdle model. This will create two models -- a logistic to determine whether bottles were purchased and a negative binomial indicating how many bottles were purchased. The predictors for the negative binomial distribution were selected based on the best poisson distribtuion discovered above (Model #4) while the logistic regression predictors were selected via a seperate regression analysis. [SEE APPENDIX]

```{r cache=TRUE}
lm.6 <- hurdle(TARGET ~ . -FixedAcidity -CitricAcid -ResidualSugar -Density -Sulphates | 
                 . -FixedAcidity -ResidualSugar -Density -Alcohol, wine.training, 
               dist='negbin')
summary(lm.6)
```

#SELECT MODELS

From the diagnostics of the models below it is clear that there is only one proper model. Model 6, the hurdle model, is not only a valid model but also accurately accounts for the disproportionate number of 0s in the training data. However, to be certain I compared the AIC, AICc and BIC values for each model. Model #6 easily performed the strongest in each of the three tests, further supporting the idea that Model #6 should be deployed. 

```{r cache=TRUE}
cbind(AIC(lm.1), AIC(lm.2), AIC(lm.3), AIC(lm.4), AIC(lm.5), AIC(lm.6)) %>%
  kable()
cbind(AICc(lm.1), AICc(lm.2), AICc(lm.3), AICc(lm.4), AICc(lm.5), AICc(lm.6)) %>%
  kable()
cbind(useBIC(lm.1), useBIC(lm.2), useBIC(lm.3), useBIC(lm.4), useBIC(lm.5), useBIC(lm.6)) %>%
  kable()
```

Examining the withheld data demonstrates that Model #6 properly predicts more than three times as many observations as any other model. 

```{r cache=TRUE, warning=FALSE}
pred.1 <- predict(lm.1, newdata=wine.testing, interval='confidence')
sum(ifelse(pred.1[, 2] < wine.testing[, 1] & pred.1[, 3] > wine.testing[, 1], 1, 0))

pred.2 <- predict(lm.2, newdata=wine.testing, interval='confidence')
sum(ifelse(pred.2[, 2] < wine.testing[, 1] & pred.2[, 3] > wine.testing[, 1], 1, 0))

pred.3 <- predict(lm.3, newdata=wine.testing, type='link', se.fit=TRUE)
pred.3.upr <- pred.3$fit + (1.96*pred.3$se.fit)
pred.3.lwr <- pred.3$fit - (1.96*pred.3$se.fit)
pred.3.data <- data_frame(fit=pred.3$fit, lwr=pred.3.lwr, upr=pred.3.upr)
sum(ifelse(exp(pred.3.data[, 2]) < wine.testing[, 1] & 
             exp(pred.3.data[, 3]) > wine.testing[, 1], 1, 0))

pred.4 <- predict(lm.4, newdata=wine.testing, type='link', se.fit=TRUE)
pred.4.upr <- pred.4$fit + (1.96*pred.4$se.fit)
pred.4.lwr <- pred.4$fit - (1.96*pred.4$se.fit)
pred.4.data <- data_frame(fit=pred.4$fit, lwr=pred.4.lwr, upr=pred.4.upr)
sum(ifelse(exp(pred.4.data[, 2]) < wine.testing[, 1] & 
             exp(pred.4.data[, 3]) > wine.testing[, 1], 1, 0))

pred.5 <- predict(lm.5, newdata=wine.testing, type='link', se.fit=TRUE)
pred.5.upr <- pred.5$fit + (1.96*pred.5$se.fit)
pred.5.lwr <- pred.5$fit - (1.96*pred.5$se.fit)
pred.5.data <- data_frame(fit=pred.5$fit, lwr=pred.5.lwr, upr=pred.5.upr)
sum(ifelse(exp(pred.5.data[, 2]) < wine.testing[, 1] & 
             exp(pred.5.data[, 3]) > wine.testing[, 1], 1, 0))

pred.6 <- predict(lm.6, newdata=wine.testing, type='prob', se.fit=TRUE)

y <- as_data_frame(pred.6) %>%
  mutate(obs = row_number()) %>%
  gather(key='prediction', value='prob', -obs) %>%
  group_by(obs) %>%
  filter(prob == max(prob)) %>%
  arrange(obs)

sum(ifelse(y == wine.testing[, 1], 1, 0))
```

Taking the total of all the information into account, there is no question that Model #6 is the best model. As such, I will be deploying this model on the evaluation data. Unlike the training data where I could remove entries with multiple missing values, I do not have that luxery with the evaluation data. I will impute everything missing.

```{r cache=TRUE, warning=FALSE, message=FALSE}
set.seed(123)
wine.evaluation <- read_csv('C:\\Users\\Brian\\Desktop\\GradClasses\\Summer18\\621\\621week5\\wine-evaluation-data.csv') %>%
  dplyr::select(-IN) %>%
  mutate(LabelAppeal = factor(LabelAppeal),
         STARS = factor(STARS))

eval.imputed.data <- mice::mice(wine.evaluation[, -1], m=5, maxit=50, 
                                method='pmm', seed=500, printFlag=FALSE)
wine.evaluation.complete <- cbind(wine.evaluation[, 1], complete(eval.imputed.data, 1))

write_csv(as_data_frame(round(predict(lm.6, newdata=wine.evaluation.complete, 
                                      type='prob'), 2)), 'evaluation_predictions.csv')
```

#Conclusion

The analysis has successfully produced a model that can accurately predict the number of cases of wine purchased. This project underscores the importance of initial data exploration prior to the development of models. The disproportionately large number of 0 case entries highlights the need for a hurdle model. In fact, after examining a variety of different models, the hurdle model performed so much better than any other model that it was the only logical choice for deployment. Had this been an actual business situation I would have jumped right the hurdle methodology.

#Appendix

##Model 1

In order to perform a multiple linear regression their must be a linear relationship between the response variable and the predictor. The presence of this relationship is hard to determine due to the small variation in the reponse variable value. However, each predictor appears to be linearly related to the reponse. 

```{r cache=TRUE, warning=FALSE}
wine.complete %>%
  gather(key='predictor', value='value', 2:15, -TARGET) %>%
  ggplot(aes(value, TARGET)) +
  geom_point() +
  geom_smooth(method='loess') +
  facet_wrap(~predictor, scales='free') +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

Examining the avPlots and the diagnostics shows that heteroscadacity is a problem with this model. As the values increase so does the variance in the residuals. This indicates that this is not a valid model. In addition, the qqplot shows that the residuals do not appear to be normally distributed and have long tails which are especially challenging for linear regression to model accurately.

```{r cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
car::avPlots(lm.1)
plot(lm.1)
```

There does not appear to be an issue with mutli-collinearity in the data so it does not appear that there are any reasonable modifications that can be made to the raw data to make the multiple linear regression fit better.

```{r cache=TRUE}
car::vif(lm.1)
```

```{r cache=TRUE, warning=FALSE}
car::mmps(lm.1)
```

Finally, the marginal model plots appear to indicate a good fit.

##Model 2

The second model attempts to further restrict the number of predictors used in order to create a more simple, interpretable model.

```{r cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
car::avPlots(lm.2)
plot(lm.2)
```

Just like with the first model, there appears to be an issue with non-constant variance and long tails in the qqplot. The anova test performed aboved indicates a reduction in predictive ability in this model compared to the first one. It appears that I may have removed too many predictors. The marginal model plots and multi-collinearity check result in similar results to model 1.

##Model 3

This model's predictor selection was performed with the lasso method.

```{r cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
set.seed(123)
lasso <- lars(model.matrix(~ . -TARGET, wine.training), wine.training$TARGET)
plot(lasso)
cvlmod <- cv.lars(model.matrix(~ . -TARGET, wine.training), wine.training$TARGET)
cvlmod$index[which.min(cvlmod$cv)]
predict(lasso, s=0.989899, type='coef', mode='fraction')$coef
```

Exploration of the model's diagnostics reveal similar issues as the mutliple linear regression model.

```{r cache=TRUE}
AER::dispersiontest(lm.3)
```

Dispersion is not an issue for this model so we will not need to adjust for that in our model.

```{r cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
summary(goodfit(wine.training$TARGET))
rootogram(table(wine.training$TARGET), fitted=table(c(trunc(fitted(lm.3)), 8)), type='hanging')
```

A goodfit test indicates that a poisson distribution is a poor choice for this model. The corresponding rootogram agrees with this assessment by underfitting predictions of 0 and overfitting predictions of 1 and 2 and 3.

##Model 4

```{r cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
summary(fit <- goodfit(wine.training$TARGET))
rootogram(table(wine.training$TARGET), fitted=table(c(trunc(fitted(lm.4)), 8)), type='hanging')
```

The modification of the predictors has not addressed the underlying issue that the model is underpredicting 0s and overpredicting 1, 2 and 3.

##Model 5

```{r cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
plot(lm.5)
rootogram(table(wine.training$TARGET), fitted=table(c(trunc(fitted(lm.5)), 8)), type='hanging')
```

The diagnostics for model 5 paint a similarly poor fit as the previous models. All of them severly underrepresent the amount of 0 cases purchased. This all indicates that this data should be modeled with a hurdle model.

##Model 6

In order to select the link functions, I performed an examination between a poisson distribution and a negative binomial distribution. The stronger performance of the negative binomial plot led me to select that as the link function.

```{r cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
distplot(wine.training$TARGET, type='poisson')
distplot(wine.training$TARGET, type='nbinomial')
```

The predictors for the logistic regression portion of the hurdle model were selected by performing a logistic regression on the data.

```{r cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
wine.bi <- wine.complete %>%
  mutate(TARGET_BI = factor(ifelse(TARGET > 0, 1, 0))) %>%
  dplyr::select(-TARGET)

lm.bi <- glm(TARGET_BI ~ ., data=wine.bi, family=binomial)
MASS::stepAIC(lm.bi, trace=0)
lm.bi <- update(lm.bi, . ~ . -FixedAcidity -ResidualSugar -Density -Alcohol)
summary(lm.bi)
```

Exploring the fit of the model yields some promising results. First, the lack of fit in the qqplot has been addressed.

```{r cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
faraway::halfnorm(residuals(lm.6))
```

The rootogram as well shows a better fit. We are over fitting on 0, 1 and 2 but not nearly as much as the previous models.

```{r cache=TRUE, fig.height=4, fig.width=4, fig.align='center'}
y <- as_data_frame(predict(lm.6, newdata=wine.training, type='prob')) %>%
  mutate(obs = row_number()) %>%
  gather(key='prediction', value='prob', -obs) %>%
  group_by(obs) %>%
  filter(prob == max(prob))

rootogram(table(wine.training$TARGET), fitted=table(c(y$prediction, 8)), type='hanging')
```


